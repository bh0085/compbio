import os
import numpy as np
import re
import itertools as it
import scipy as sp
import scipy.linalg as lin
import matplotlib.pyplot as plt
from scipy import sparse
from numpy import *
import pickle
import subprocess
import random
import mlpy
import compbio.utils.colors as mycolors
import netwriter as nw

default_name = 'fRN'

def parse_net(name = default_name,number = 0):
    prefix = os.path.join('predmodel/regressionwts', name)
    nwdata = open(os.path.join(prefix,
                               'nw_'+str(number) + '.sif')).read()
    #A few functions defined here to be used later
    trgfun = lambda x: x[1]
    wtfun = lambda x:float( x[2] )
    tffun = lambda x: x[0]
    sigmafun = lambda x: 1 / (1 + np.exp(-x /1))

    r = re.compile('^[ ]*(?P<target>[^\s]+)\t(?P<tf>[^\s]+)\t(?P<weight>[^\s]+)'
                   ,re.M)
    matches = list(re.finditer(r,nwdata))    
    #Unsorted lists of tfs and targets
    targets =map(lambda x:x.group('target'),matches)
    tfs =    map(lambda x:x.group('tf'),matches)
    weights =map(lambda x:x.group('weight'),matches)
    
    #Concat the data for easier sorting
    cat = []
    for i in np.argsort(tfs):
        cat.append([tfs[i],targets[i],weights[i]])

    #Extract a dictionary with information for each target.
    trg_d = {}
    count = 0.0
    for k, g in it.groupby(sorted(cat,key = trgfun),key = trgfun):
        l = list(g)
        count += 1.0
        trg_d[k] = {'color': np.array([count, 0, 0]),
                    'tfs' : map(tffun,l),
                    'weights': map(wtfun,l)
                    }


    #Extract a dictionary with information for each TF
    tf_d = {}
    for k, g in it.groupby(cat,key = lambda x: x[0]):
        l = list(g)
        tf_targets = map(lambda x: x[1],l)
        
        tf_d[k] = {'targets':map(trgfun,l),
                   'weights':map(wtfun,l)}

    return (trg_d, tf_d)


def parse_TS():
    f = open('TC.geneexp').read()
    elts =f.split('\n')
    seqdict = {}
    for e in elts:
        matches = list(re.finditer(re.compile('([^\s]+)'), e))
        if not len(matches): continue
        name = matches[0].group(1)
        seqdict[name] = []
        for i in matches[1:]:
            seqdict[name].append(float(i.group(1)))

    pickle.dump(seqdict,open('TC.pickle','w'))

def parse_CL():
    f = open('CL.geneexp').read()
    elts =f.split('\n')
    seqdict = {}
    for e in elts:
        matches = list(re.finditer(re.compile('([^\s]+)'), e))
        if not len(matches): continue
        name = matches[0].group(1)
        seqdict[name] = []
        for i in matches[1:]:
            seqdict[name].append(float(i.group(1)))

    pickle.dump(seqdict,open('CL.pickle','w'))

def load_CL():
    f = pickle.load(open('CL.pickle'))
    return f
def load_TS():
    f = pickle.load(open('TC.pickle'))
    return f


import pickle
import inspect

last_square =  100




#RESETS:
#reset value specifies both whether a function
#will recompute its value (reset > 0)
#and whether it will ask its children to recompute (reset mod 2)
def net_square_affinity(name = default_name, reset = 0):

    hardcopy = True
    try:
        if reset: raise Exception('compute')
        return nw.readnet(name,hardcopy = hardcopy, np = np)
    except Exception as e:
        if e.args[0] != 'compute': raise Exception()
        claim_reset()
        trgs, tfs = parse_net(name)
        k0 = trgs.keys()
        k1 = tfs.keys()
    

        all_keys = k0
        all_keys.extend(k1)
        all_keys = sort(all_keys)
        uk = []
        for g,v in it.groupby(all_keys): uk.append(g)
        nk = size(uk)
        N =zeros((nk,nk),float32)

        tf_imap = {}
        for k in tfs.keys():
            tf_imap[k] = uk.index(k)

        for k,t in trgs.iteritems():
            i = uk.index(k)
            for ktf in t['tfs']:
                j = tf_imap[ktf]
                N[i,j] = 1
        
            
        affinity = N
        nw.writenet(name, affinity,hardcopy = hardcopy,np = True)
        return affinity
    

def net_affinity(name = default_name ,reset = 0):
    hardcopy = True
    try:
        if reset: raise Exception('compute')
        return nw.readnet(name, hardcopy = hardcopy, np = True)
    except Exception as e:
        if e.args[0] != 'compute': raise Exception()
        claim_reset()
        trgs, tfs = parse_net(name)
        k0 = trgs.keys()
        k1 = tfs.keys()
    
        kmap0 = {}
        kmap1 = {}
        for i in range(len(k0)):
            kmap0[k0[i]] = i
        for i in range(len(k1)):
            kmap1[k1[i]] = i

    #Now, for a first shot, lets construct an n*m matrix
    #with possible interactions for each gene appearing in 
    #the network        
    #prune the network for testing purposes.
        m = len(kmap0.keys())
        
        n = len(kmap1.keys())
        N = np.zeros([m,n],float32)
        for k_trg,v_trg in trgs.items():
            i = kmap0[k_trg]
            for k_tf in v_trg['tfs']:            
                j = kmap1[k_tf]
                N[i][j] = 1.0

        affinity =N
        nw.writenet(name, affinity,hardcopy = hardcopy, np = True)
        return affinity

def net_jcf(name = default_name, reset = 0):
    hardcopy = False
    try:
        if reset: raise Exception('compute')
        return nw.readnet(name,hardcopy = hardcopy)
    except Exception as e:
        if e.args[0] != 'compute': raise Exception()
        claim_reset()

        square_aff = net_square_affinity(name, reset = mod(reset,2))
        import pymat
        command = 'pymatjordan'
        arr = square_aff[0]
        
        inp = {'array':arr}
        output = pymat.runmat(command, inp)
        nw.writenet(name, output, hardcopy = hardcopy)
        return output

    



def net_genegene(name = default_name, reset = 0):
    hardcopy = True
    try:
        if reset: raise Exception('compute')
        return nw.readnet(name,hardcopy = hardcopy, np = True)
    except Exception as e:
        if e.args[0] != 'compute': raise Exception()
        claim_reset()
        sq = net_square_affinity(name, reset = mod(reset,2))[0]
        genegene = dot(sq,sq.T)
        nw.writenet(name,genegene, hardcopy = hardcopy, np =True)
        return genegene

def net_genegene_norm(name = default_name, reset = 0):
    hardcopy = True
    try:
        if reset: raise Exception('compute')
        return nw.readnet(name,hardcopy = hardcopy,np = True)
    except Exception as e:
        if e.args[0] != 'compute': raise Exception()
        claim_reset()
        sq0 =array(net_square_affinity(name, reset = mod(reset,2))[0]) 
        sq1 =array( sq0.T)
        n = shape(sq0)[0]

        for i in range(n):
            sq0[:,i] /= max(1,np.sum(sq0[:,i]))
            sq1[:,i] /= max(1,sp.sum(sq1[:,i]))
        genegene = dot(sq0,sq1)
        nw.writenet(name,genegene, hardcopy = hardcopy,np = True)
        return genegene


def net_hcluster_genegene_norm(name = default_name, reset = 0, n = 200,
                              cosine = True,
                              hier = True,
                              cut = .5):
    hardcopy = True
    try:
        if reset: raise Exception('compute')
        return nw.readnet(name,hardcopy =hardcopy)
    except Exception as e:
        if e.args[0] != 'compute': raise Exception()
        claim_reset()
        sqa = net_genegene_norm(name, reset = mod(reset,2))
        gg = sqa
        sub = matrix(gg[0:n,0:n])


        if cosine:
            for view in sub:
                view = view / sum(view)
        
        h = mlpy.HCluster(link = 'complete')
        comp = h.compute(sub)
        clustered = h.cut(cut)
   
        value = {'HCluster':h,'clusters':clustered}
        nw.writenet(name,value, hardcopy = hardcopy)
        return value
        
def net_kcluster_genegene_norm(name = default_name, reset = 0, n = 200, 
                               k = 5,
                               cosine = True):
    hardcopy = True
    try:
        if reset: raise Exception('compute')
        return nw.readnet(name,hardcopy =hardcopy)
    except Exception as e:
        if e.args[0] != 'compute': raise Exception()
        claim_reset()
        sqa = net_genegene_norm(name, reset = mod(reset,2))
        gg = sqa
        sub = matrix(gg[0:n,0:n])


        if cosine:
            for view in sub:
                view = view / sum(view)


        
        kmeans = mlpy.Kmeans(k)
        clustered = kmeans.compute(sub)
        means = kmeans.means

        value = {'KMeans':kmeans,'clusters':clustered}
        nw.writenet(name,value, hardcopy = hardcopy)
        return value
        


def net_blanket( name = default_name , reset = 0, order = 1):


    saff = net_square_affinity(name,reset = mod(reset,2))

    m = saff[0]
    n = shape(m)[0]
    for i in range(n):
        m[i,i]  = 1
        

    m2 = m * m.T
    lower100 = m2[0:500,0:500].todense()
    f = plt.figure(0)
    f.clear()
    ax = f.add_axes([0,0,1,1], aspect = 'auto',frameon=False)
    ax.imshow(lower100,interpolation = 'nearest')

    
    return


def expr_TS_binary(reset = 0 , 
                   hardcopy = True ):
    pass

def expr_CL_binary(reset = 0,
                   hardcopy = True):
    ts = load_CL()
    ts_mixtures = {}
    count = 0
    


    mean0,max0,mean1,max1,n0,n1 = [[] for i in range(6)]
    for k in ts.keys():
        if count > 100: break
        count += 1
        t = ts[k]
        #ts_mixtures contains the probabilities of a classification
        #at each timepoint (0 = off, 1 = onn)
        mixture = expr_gmm_onoff(t)
        cutoff = .25 #demand 25% certainty for a label
        mix0 = squeeze(mixture[:,0])
        mix1 = squeeze(mixture[:,1])

        states = zeros(

        for i in range(3):
            if i == 0: w = nonzero(greater(mix0, mix1+cutoff))[0]
            elif i == 1: w = nonzero(greater(mix1, mix0+cutoff))[0]
            elif i == 2: w = nonzero(less(abs(mix0 - mix1),cutoff))[0]
                     
            n = len(w)
            if i < 2: 
                if n == 0: meanu =1
                else: meanu =mean( [mixture[w,i] -mixture[w,1-i]])
            if i < 2: 
                if n == 0: maxu = 1
                else: maxu =np.max([  mixture[w,i] - mixture[w,i-1]])


            if i ==0:
                mean0.append(meanu)
                max0.append(maxu)
                n0.append(n)
            elif i==1:
                mean1.append(meanu)
                max1.append(maxu)
                n1.append(n)

    
    

def plot_binary_stats(mean0,max0,n0, mean1,max1,n1):
     
    max1 = array(max1)
    mean1 = array(mean1)
    max0 = array(max0)
    mean0 = array(mean0)
    n0 = array(n0)
    n1 = array(n1)
    n = len(mean1)
    f = plt.figure(0)
    f.clear()
    ax = f.add_axes([.05,0,.95,1])
    xax = arange(n)
    ct = mycolors.getct(2)

    nt = array(n0) + array(n1)
    srt = argsort(nt)


    #ax.plot(xax,mean0[srt],color =ct[0])
    #ax.plot(xax,max0[srt],color = ct[0])
    ax.plot(xax,mean1[srt],color = ct[1])
    ax.plot(xax,max1[srt],color = ct[1])

    f2 = plt.figure(1)
    f2.clear()
    ax2 = f2.add_axes([.05,0,.95,1])
    srt = argsort(nt)
    xax = range(len(nt))
    ax2.plot(xax,nt[srt])
    ax2.plot(xax,n0[srt])
    ax2.plot(xax,n1[srt])

def expr_view():
    ts = load_TS()
    
    saff = net_square_affinity()
    raff = net_affinity()

    tfkeys = raff[2]
    skeys = saff[1]

    sqidxs = []
    for k in tfkeys.keys():
        sqidxs.append(skeys.index(k))

    f = plt.figure(1)
    f.clear()
    ax = f.add_axes([0,0,1,1])
    for k in tfkeys:
        
        expr_gmm_onoff(ts[k],draw = True)
        expr_gmm_onoff(ts[k],log_expr = False,draw =True, fig = 2)

        inp = raw_input('next')
        if inp == 'd':
            break
        
def expr_gmm_onoff(expr_in,
                   log_expr = True, 
                   fig = 1,
                   draw = False):
    
    expr = (array(expr_in))
    dev = std(expr)
    if log_expr:
        expr = log(expr + dev)
    n = len(expr)
    expr_array = zeros((n,1))
    for i in range(n):
        expr_array[i] = expr[i]
    expr = expr_array
        
    from scikits.learn import gmm
    
    #demand seperation of max from alternate hypotheses by e/2
    cmin_diff = log(e*1.5)
    #cmin_diff = .0001
    k = 2

    G = gmm.GMM(n_states = k, n_dim = 1)
    G.fit(expr)
    [probs, clusters] = G.decode(expr)
    [probs, mixtures] = G.eval(expr)
    mean_as = argsort(G.means,0)
    for i in range(shape(mixtures)[0]):
        mixtures[i,:] = mixtures[i,squeeze(mean_as)]

    
    if draw:
        n = len(expr)
        xax = arange(n)[argsort(expr,0)]
        f = plt.figure(fig)
        f.clear()
        ax = f.add_axes([0,0,1,1])

        ct =mycolors.getct(k)
        cs, rs = [], []

        c2s  = []
        r2s = []
        x2s = []
        y2s = []
        for i in range(n):
            cs.append(ct[mean_as[clusters[i]]])
            rs.append(100)
            for j in range(k):
                mprob = mixtures[i,j]
                x2s.append(i)
                y2s.append(expr[i])
                c2s.append(ct[j])
                r2s.append(pow(exp(mprob),2)*100)
           
        x3s,y3s,c3s,r3s = [], [], [], []
        for i in range(n):
            probs = mixtures[i,:]
            cval = clusters[i]
            srt = argsort(probs)[::-1]
            maxval = probs[srt[0]]
            secval = probs[srt[1]]
            reliable = False
    
            if log(maxval) - log(secval) > cmin_diff: reliable = True

            x3s.append(i)
            y3s.append(expr[i])
            r3s.append(200)
            if not reliable:
                color = [0,0,0]
            else:
                color = ct[srt[0]]
            c3s.append(color)
    #ax.scatter(xax,expr,rs, color = cs)
        ax.scatter(x2s,y2s,r2s,edgecolor=c2s,facecolor = 'none')   
        ax.scatter(x3s,y3s,r3s,c3s)
    return mixtures

def expr_getonoff(expr_in):
    expr = array(expr_in)
    dev = std(expr)
    #expr = log(expr+dev)
    k = 3
    km = mlpy.Kmeans(k)
    n = len(expr)
    expr_2d = []
    for i in range(n):
        expr_2d.append(array([expr[i],0]))
    expr_2d = array(expr_2d)
    comp = km.compute(expr_2d)
    means = km.means
    compsort = arange(k)[argsort(map(lambda x: x[0],means))]

    n = len(expr)
    xax = argsort(expr)
    
    means = zeros(k)
    stds = zeros(k)
    for i in range(k):
        idxs = nonzero(equal(comp,i))[0]
        vals = array(expr)[idxs]
        means[i] = mean(vals)
        stds[i] = std(vals)
        

    f = plt.figure(1)
    f.clear()
    ax = f.add_axes([0,0,1,1])

    ct =mycolors.getct(k)
    cs, rs = [], []
    for i in range(n):
        cs.append(ct[compsort[comp[i]]])
        rs.append(100)
    ax.scatter(xax,expr,rs, color = cs)
    

    x0 = 0
    y0 = 0
    for i in range(k):
        ax.plot([x0,x0],[means[i] - stds[i], means[i]+ stds[i]]
                ,linewidth = 5, color = ct[compsort[i]])





def nsq_keys(name = default_name, reset = 0 ):
    hardcopy = True
    try:
        if reset: raise Exception('compute')
        return nw.readnet(name,hardcopy = hardcopy)
    except Exception as e:
        if e.args[0] != 'compute': raise Exception()
        claim_reset()    
        keys = net_square_affinity(name)[1]
        #store keys in a dict for easy comparison
        keydict = {}
        for i in range(len(keys)):
            keydict[keys[i]] = i

        nw.writenet(name, keydict,hardcopy = hardcopy)
        return keydict

def nsq_gkeys(name = default_name, reset = 0 ):
    hardcopy = True

    try:
        if reset: raise Exception('compute')
        return nw.readnet(name,hardcopy = hardcopy)
    except Exception as e:
        if e.args[0] != 'compute': raise Exception()
        claim_reset()    
        keys = nsq_keys(name)
        gkeys = na_gkeys(name)
        sqg = {}
        for gk in gkeys.keys():
            sqg[gk] = keys[gk]


        nw.writenet(name, sqg,hardcopy = hardcopy)
        return sqg

def nsq_tfkeys(name = default_name, reset = 0 ):
    hardcopy = True
    try:
        if reset: raise Exception('compute')
        return nw.readnet(name,hardcopy = hardcopy)
    except Exception as e:
        if e.args[0] != 'compute': raise Exception()
        claim_reset()    
        keys = nsq_keys(name)
        tfkeys = na_tfkeys(name)
        sqtf = {}
        for tfk in tfkeys.keys():
            sqtf[tfk] = keys[tfk]
        nw.writenet(name, sqtf,hardcopy = hardcopy)
        return sqtf

def na_gkeys(name = default_name, reset = 0 ):
    hardcopy = True

    try:
        if reset: raise Exception('compute')
        return nw.readnet(name,hardcopy = hardcopy)
    except Exception as e:
        if e.args[0] != 'compute': raise Exception()
        claim_reset()    
        keys = net_affinity()[1]
        nw.writenet(name, keys,hardcopy = hardcopy)
        return keys


def na_tfkeys(name = default_name, reset = 0 ):
    hardcopy = True
    try:
        if reset: raise Exception('compute')
        return nw.readnet(name,hardcopy = hardcopy)
    except Exception as e:
        if e.args[0] != 'compute': raise Exception()
        claim_reset()    
        keys = net_affinity()[2]

        nw.writenet(name, keys,hardcopy = hardcopy)
        return keys


#... default ordering for TFs
#right now, they are ordered simply by
#the order of their out degree
def tf_ordering(name = default_name, reset = 0 ):
    hardcopy = True
    try:
        if reset: raise Exception('compute')
        return nw.readnet(name,hardcopy = hardcopy)
    except Exception as e:
        if e.args[0] != 'compute': raise Exception()
        claim_reset()

        #default keymap into tfspace
        keymap = na_tfkeys()
        aff = net_affinity()[0]
        deg_sort = np.argsort(np.sum(aff,0))[::-1]
        
        kordered = {}
        count = 0
        for d in deg_sort:
            k = keymap.keys()[keymap.values().index(d)]
            kordered[k] = count
            count+=1

        #this way is faster but who cares?
        #klist = []
        #kvpairs = map(lambda x,y:[x,y], keymap.iteritems())
        #vals = map(lambda x: x[1],kvpairs)
        #vals_srt = argsort(vals)
        #keys = map(lambda x: x[0],kvpairs)
        #keys_srt = keys[vals_srt]
        
        
        
        nw.writenet(name, kordered,hardcopy = hardcopy)
        return kordered



#... default ordering for genes
#right now, they are ordered simply by
#the order of their in degree
def gene_ordering(name = default_name, reset = 0 ):
    hardcopy =True
    try:
        if reset: raise Exception('compute')
        return nw.readnet(name,hardcopy = hardcopy)
    except Exception as e:
        if e.args[0] != 'compute': raise Exception()
        claim_reset()

        #default keymap into tfspace
        keymap = na_gkeys()
        aff = net_affinity()[0]
        deg_sort = np.argsort(np.sum(aff,1))[::-1]
        
        kordered ={}
        count = 0
        for d in deg_sort:
            k = keymap.keys()[keymap.values().index(d)]
            kordered[k] = count
            count+=1

        #this way is faster but who cares?
        #klist = []
        #kvpairs = map(lambda x,y:[x,y], keymap.iteritems())
        #vals = map(lambda x: x[1],kvpairs)
        #vals_srt = argsort(vals)
        #keys = map(lambda x: x[0],kvpairs)
        #keys_srt = keys[vals_srt]
        
        
        
        nw.writenet(name, kordered,hardcopy = hardcopy)
        return kordered


def nsq_tfidxs(name = default_name, reset = 0 ):
    try:
        if reset: raise Exception('compute')
        return nw.readnet(name)
    except Exception as e:
        if e.args[0] != 'compute': raise Exception()
        claim_reset()

        #default keymap into tfspace
        keymap = tf_ordering(name)
        n = len(keymap.keys())

        #keymap into sq_tfspace
        keys = nsq_tfkeys(name)
        idxs = zeros(n,int)

        #compute idxmap from tfspace into sq_tfspace
        for k,v in keymap.iteritems():
            idxs[v] = keys[k]
            
        nw.writenet(name, idxs)
        return idxs

def na_tfidxs(name = default_name, reset = 0 ):
    try:
        if reset: raise Exception('compute')
        return nw.readnet(name)
    except Exception as e:
        if e.args[0] != 'compute': raise Exception()
        claim_reset()

        #default keymap into tfspace
        keymap = tf_ordering(name)
        n = len(keymap.keys())

        #keymap into sq_tfspace
        keys = na_tfkeys(name)
        idxs = zeros(n,int)

        #compute idxmap from tfspace into sq_tfspace
        for k,v in keymap.iteritems():
            idxs[v] = keys[k]
            
        nw.writenet(name, idxs)
        return idxs

def nsq_gidxs(name = default_name, reset = 0 ):
    try:
        if reset: raise Exception('compute')
        return nw.readnet(name)
    except Exception as e:
        if e.args[0] != 'compute': raise Exception()
        claim_reset()

        #default keymap into tfspace
        keymap = gene_ordering(name)
        n = len(keymap.keys())

        #keymap into sq_tfspace
        keys = nsq_gkeys(name)
        idxs = zeros(n,int)

        #compute idxmap from tfspace into sq_tfspace
        for k,v in keymap.iteritems():
            idxs[v] = keys[k]
            
        nw.writenet(name, idxs)
        return idxs

def na_gidxs(name = default_name, reset = 0 ):
    try:
        if reset: raise Exception('compute')
        return nw.readnet(name)
    except Exception as e:
        if e.args[0] != 'compute': raise Exception()
        claim_reset()

        #default keymap into tfspace
        keymap = gene_ordering(name)
        n = len(keymap.keys())
        #keymap into sq_tfspace
        keys = na_gkeys(name)
        idxs = zeros(n,int)

        #compute idxmap from tfspace into sq_tfspace
        for k,v in keymap.iteritems():
            idxs[v] = keys[k]
            
        nw.writenet(name, idxs)
        return idxs
